
# 表示学习

by SwWu, NUSTM

** 本材料使用Numpy包，搭建了传统机器学习的词向量表示学习算法。对传统的表示学习算法感兴趣的同学，本材料可以帮助你们系统的了解传统机器学习的表示学习算法的原理以及实现的细节。**这里不会提供相关的知识，只是梳理一下学习的流程和材料。

## 程序内容

每个词向量学习算法的程序分为两类：基于**标量**形式的推导过程进行程序编写和基于**向量**形式推导过程进行程序的编写。此外，为了验证推导过程的正确性，对两种推导形式的代码，分别提供了在给定的小规模数据运行的代码，从而验证标量推导和向量推导的**一致性**。

- "XXX_main"：基于向量推导过程在语料上运行的代码（**已封装**）
- "XXX_part_main"：基于向量推导过程在给定的小规模数据上运行的代码
- "XXX_标量_main"：基于标量推导过程在语料上运行的代码
- "XXX_标量_part_main"：基于标量推导过程在给定的小规模数据上运行的代码

**PS：Glove、Hierarchical Softmax以及NEG(Negative Sample)都只提供了一个程序--用于在语料上训练词向量。**
所有的"XXX_main"函数的子程序都进行了**封装**，并实现了**Word Similarity任务**，计算了**皮尔斯系数**，**通过下游任务来验证词向量训练的好坏**。

## 下游任务上的结果

每个算法都使用了20组随机数跑了**Word Similarity任务**。"rtho.txt"文件保存的是20次计算的皮尔斯系数，"Analyse_rtho.txt"里保存的是20次试验的平均值和方程。与**gensim包**中的CBOW_HF以及skip-gram-NEG算法进行了比较，在同一预料下，进行了20组随机数的实验，跑了**Word Similarity任务**，比较结果如下：

+----------------------+-----------------+-----------------+
|      Name\rtho       |     Mean        |        Var      |
+----------------------+-----------------+-----------------+
|      CBOW_HS_our     |    0.2681439    |   0.000682719   |
|   skip-gram_NEG_our  |    0.2738680    |   0.002115254   |
|  CBOW_HS_our_gensim  |    0.2911922    |   0.000595353   |
| skip-gram_NEG_gensim |    0.1124676    |   0.000987214   |
+----------------------+-----------------+-----------------+

### 学习内容包括：

1. NNLM
2. Log-Bilinear
3. C&W
4. CBOW 
5. skip-gram
6. Hierarchical Softmax
7. NEG(Negative Sample)
8. Glove